{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning with Optuna\n",
    "\n",
    "This script was used for tuning hyperparameters with Optuna, using https://github.com/optuna/optuna-examples/blob/main/pytorch/pytorch_simple.py as a guide for the objective function. Important considerations are marked by `NOTE` in the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"mps\")\n",
    "BATCHSIZE = 512\n",
    "CLASSES = 10\n",
    "DIR = os.getcwd() #./datafiles/\n",
    "EPOCHS = 20\n",
    "N_TRAIN_EXAMPLES = BATCHSIZE * 30\n",
    "N_VALID_EXAMPLES = BATCHSIZE * 10\n",
    "\n",
    "LOSS_FN = nn.CrossEntropyLoss()\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist():\n",
    "\n",
    "    # Any data augmentation should be added to training\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomRotation(15), #NOTE: this was commented out for the \"no data augmentation runs\"\n",
    "        transforms.RandomAffine(25), #NOTE: this was commented out for the \"no data augmentation runs\"\n",
    "        transforms.ElasticTransform(alpha=70.0), #NOTE: this was commented out for the \"no data augmentation runs\"\n",
    "        transforms.Normalize(mean=0.1307, std=0.3081),\n",
    "    ])\n",
    "\n",
    "    # Test data should have normalization applied, but no augmentation\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=0.1307, std=0.3081)\n",
    "    ])\n",
    "\n",
    "    # Load FashionMNIST dataset.\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(DIR, train=True, download=True, transform=transforms.ToTensor()),\n",
    "        batch_size=BATCHSIZE,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(DIR, train=False, download=True, transform=transforms.ToTensor()),\n",
    "        batch_size=BATCHSIZE,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def get_output_size(input_size, padding, stride, kernel):   \n",
    "    return math.floor((input_size + 2*padding - kernel)/stride) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Model(nn.Module):\n",
    "    def __init__(self, trial):\n",
    "        super().__init__()\n",
    "\n",
    "        img_size = 28\n",
    "\n",
    "        # NOTE: optuna params: 1st layer output could be 8 16 32\n",
    "        layer1_channels_exp = trial.suggest_int(\"layer1_channels_exp\", 3, 5)\n",
    "        # NOTE: optuna params: 2nd layer output could be 64 128 256\n",
    "        layer2_channels_exp = trial.suggest_int(\"layer2_channels_exp\", 6, 8)\n",
    "\n",
    "        layer1_channels = 2 ** layer1_channels_exp\n",
    "        layer2_channels = 2 ** layer2_channels_exp\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, layer1_channels, kernel_size=(5, 5), padding='same')\n",
    "        self.conv2 = nn.Conv2d(layer1_channels, layer2_channels, kernel_size=(3, 3), padding='same')\n",
    "        self.mp = nn.MaxPool2d(kernel_size=(2, 2), stride=2, padding=1)\n",
    "        \n",
    "        # Batch nrom\n",
    "        self.bn1 = nn.BatchNorm2d(layer1_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(layer2_channels)\n",
    "\n",
    "        # get output size for linear layer since it is varied each trial\n",
    "        out1 = get_output_size(input_size=img_size, padding=1, stride=2, kernel=2)\n",
    "        out2 = get_output_size(input_size=out1, padding=1, stride=2, kernel=2)\n",
    "\n",
    "        # Activation\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        p = trial.suggest_float(\"dropout_p:\", 0, 0.1) #NOTE: optuna param\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.output_layer = nn.Linear(layer2_channels*out2*out2, CLASSES)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.mp(x)\n",
    "        x = self.bn1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.mp(x)\n",
    "        x = self.bn2(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(x.size(0), -1) \n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "def define_model(trial):\n",
    "    model = MNIST_Model(trial)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # NOTE: Credit to https://github.com/optuna/optuna-examples/blob/main/pytorch/pytorch_simple.py \n",
    "    # for providing this training loop example code, which we used with our own added metrics and\n",
    "    # other small changes for the experimentation\n",
    "    \n",
    "    # Generate the model.\n",
    "    model = define_model(trial).to(DEVICE)\n",
    "\n",
    "    # Generate the optimizers.\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\"]) #NOTE: Optuna optim type param\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-4, 1e-2, log=True) #NOTE: Optuna weight decay param.\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True) #NOTE: Optuna learning rate param\n",
    "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # Get the FashionMNIST dataset.\n",
    "    train_loader, valid_loader = get_mnist()\n",
    "    \n",
    "    # Initialize variables for tracking the best accuracy and the number of epochs since improvement\n",
    "    best_patience = 0\n",
    "    epochs_since_improvement = 0\n",
    "    best_accuracy = 0  # Initialize to 0 for accuracy\n",
    "    best_epoch = -1\n",
    "\n",
    "    # Training of the model.\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            # Limiting training data for faster epochs.\n",
    "            if batch_idx * BATCHSIZE >= N_TRAIN_EXAMPLES:\n",
    "                break\n",
    "\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = LOSS_FN(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation of the model.\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, target) in enumerate(valid_loader):\n",
    "                # Limiting validation data.\n",
    "                if batch_idx * BATCHSIZE >= N_VALID_EXAMPLES:\n",
    "                    break\n",
    "                data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "                output = model(data)\n",
    "                # Get the index of the max log-probability.\n",
    "                pred = output.argmax(dim=1, keepdim=True)\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        accuracy = correct / min(len(valid_loader.dataset), N_VALID_EXAMPLES)\n",
    "        \n",
    "        # NOTE: Added to track if early stopping is necessary\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy \n",
    "            best_epoch = epoch\n",
    "            if epochs_since_improvement > 0:\n",
    "                if best_patience < epochs_since_improvement:\n",
    "                    best_patience = epochs_since_improvement\n",
    "                epochs_since_improvement = 0\n",
    "        else:\n",
    "            epochs_since_improvement += 1\n",
    "\n",
    "        trial.report(accuracy, epoch)\n",
    "\n",
    "        # Handle pruning based on the intermediate value.\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    # NOTE: these were added for tracking early stopping\n",
    "    print(f\"Best patience value: {best_patience}\")\n",
    "    trial.set_user_attr('best_epoch', best_epoch)\n",
    "    trial.set_user_attr('best_accuracy', best_accuracy)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "# 200 trials were done with no data augmentation, then another 200 trials WITH data augmenation\n",
    "study.optimize(objective, n_trials=200, timeout=600)\n",
    "\n",
    "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "for key, value in trial.user_attrs.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignment2-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
