{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "GOAL: 99% **TEST** ACCURACY\n",
    "'''\n",
    "\n",
    "# explore different architectures, data augmentation and regularization methods to determine a suitable range of parameters\n",
    "# TODO: use optuna to explore the hyperparameter spacej\n",
    "# TODO: report final hyperparameter values and test accuracy\n",
    "\n",
    "# SUBMIT:\n",
    "# TODO: \"informed discussion\" of approach to hyperparameter exploration / observations\n",
    "# TODO: submit use of optuna code\n",
    "# TODO: final performance of model\n",
    "# TODO: discussion of limitations of not using a separate validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"mps\")\n",
    "BATCHSIZE = 512\n",
    "CLASSES = 10\n",
    "DIR = os.getcwd() #./datafiles/\n",
    "EPOCHS = 10\n",
    "N_TRAIN_EXAMPLES = BATCHSIZE * 30\n",
    "N_VALID_EXAMPLES = BATCHSIZE * 10\n",
    "\n",
    "LOSS_FN = nn.CrossEntropyLoss()\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist():\n",
    "\n",
    "    # Any data augmentation should be added to training\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=0.1307, std=0.3081),\n",
    "    ])\n",
    "\n",
    "    # Test data should have normalization applied, but no augmentation\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=0.1307, std=0.3081)\n",
    "    ])\n",
    "\n",
    "    # Load FashionMNIST dataset.\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(DIR, train=True, download=True, transform=transforms.ToTensor()),\n",
    "        batch_size=BATCHSIZE,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(DIR, train=False, download=True, transform=transforms.ToTensor()),\n",
    "        batch_size=BATCHSIZE,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def get_output_size(input_size, padding, stride, kernel):   \n",
    "    return math.floor((input_size + 2*padding - kernel)/stride) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Model(nn.Module):\n",
    "    def __init__(self, trial):\n",
    "        super().__init__()\n",
    "\n",
    "        img_size = 28\n",
    "\n",
    "        # NOTE: optuna params \n",
    "        layer1_channels_exp = trial.suggest_int(\"layer1_channels_exp\", 3, 5) # 1st layer output could be 8 16 32\n",
    "        layer2_channels_exp = trial.suggest_int(\"layer2_channels_exp\", 6, 8) # 2nd layer output could be 64 128 256\n",
    "\n",
    "        layer1_channels = 2 ** layer1_channels_exp\n",
    "        layer2_channels = 2 ** layer2_channels_exp\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, layer1_channels, kernel_size=(5, 5), padding='same')\n",
    "        self.conv2 = nn.Conv2d(layer1_channels, layer2_channels, kernel_size=(3, 3), padding='same')\n",
    "        self.mp = nn.MaxPool2d(kernel_size=(2, 2), stride=2, padding=1)\n",
    "        \n",
    "        # Batch nrom\n",
    "        self.bn1 = nn.BatchNorm2d(layer1_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(layer2_channels)\n",
    "\n",
    "        out1 = get_output_size(input_size=img_size, padding=1, stride=2, kernel=2)\n",
    "        out2 = get_output_size(input_size=out1, padding=1, stride=2, kernel=2)\n",
    "\n",
    "        # Activation\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        p = trial.suggest_float(\"dropout_p:\", 0, 0.1) #NOTE: optuna param\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.output_layer = nn.Linear(layer2_channels*out2*out2, CLASSES)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.mp(x)\n",
    "        x = self.bn1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.mp(x)\n",
    "        x = self.bn2(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(x.size(0), -1) \n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "def define_model(trial):\n",
    "    model = MNIST_Model(trial)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model_old(trial):\n",
    "    # We optimize the number of layers, hidden units and dropout ratio in each layer.\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 2, 4)\n",
    "    layers = []\n",
    "\n",
    "    in_channels = 1\n",
    "    for i in range(n_layers):\n",
    "        out_channels = trial.suggest_int(\"n_units_l{}\".format(i), in_channels, 128)\n",
    "        kernel_size = 7 - (2*i)\n",
    "        print(f\"kernel_size: {kernel_size}\")\n",
    "\n",
    "        # NOTE: Maria -> could vary kernel and padding more\n",
    "        layers.append(nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            padding='same'\n",
    "        ))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.MaxPool2d(kernel_size=(2, 2), stride=2, padding=1))\n",
    "        # Batch Norm\n",
    "        layers.append(nn.BatchNorm2d(out_channels)) #NOTE: could play around with the placement\n",
    "\n",
    "        print(f\"in_channels: {in_channels} out_channels: {out_channels}\")\n",
    "        in_channels = out_channels\n",
    "    \n",
    "    # TODO: Maria: Figure this out\n",
    "    # use_average_pool = trial.suggest_int(\"use_average_pool\", 0, 1) #NOTE: Maria: ask trevor about this?\n",
    "    # if use_average_pool:\n",
    "    #     layers.append(nn.AdaptiveAvgPool2d(output_size=(1,1)).squeeze()) #NOTE: Maria: this does not work\n",
    "\n",
    "    # n_linear_layers = trial.suggest_int(\"n_linear_layers\", 1, 2)\n",
    "    p = trial.suggest_float(\"dropout_l{}\".format(i), 0.05, 0.25) #NOTE: Zach: Verify p.\n",
    "\n",
    "    # TODO: delete this... there is still an error with this version but seems like a more important one to solve\n",
    "    layers.append(nn.Dropout(p))\n",
    "    layers.append(nn.Linear(in_channels, CLASSES))\n",
    "\n",
    "    # if n_linear_layers == 1:\n",
    "    #     layers.append(nn.Dropout(p))\n",
    "    #     layers.append(nn.Linear(in_channels, CLASSES))\n",
    "        \n",
    "    # if n_linear_layers == 2:\n",
    "    #     intermediate_channels = in_channels/2\n",
    "    #     layers.append(nn.Linear(in_channels, intermediate_channels))\n",
    "    #     layers.append(nn.Dropout(p))\n",
    "    #     layers.append(nn.Linear(intermediate_channels, CLASSES))\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Generate the model.\n",
    "    model = define_model(trial).to(DEVICE)\n",
    "\n",
    "    # TODO Zach: Add optimizer, and weight decay.\n",
    "    # Generate the optimizers.\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\"])\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-4, 1e-2, log=True) #NOTE: Optuna weight dexay param.\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
    "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # Get the FashionMNIST dataset.\n",
    "    train_loader, valid_loader = get_mnist()\n",
    "    \n",
    "    # Initialize variables for tracking the best accuracy and the number of epochs since improvement\n",
    "    best_patience = 0\n",
    "    epochs_since_improvement = 0\n",
    "    best_accuracy = 0  # Initialize to 0 for accuracy\n",
    "    best_epoch = -1\n",
    "\n",
    "    # Training of the model.\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            # Limiting training data for faster epochs.\n",
    "            if batch_idx * BATCHSIZE >= N_TRAIN_EXAMPLES:\n",
    "                break\n",
    "\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = LOSS_FN(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation of the model.\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, target) in enumerate(valid_loader):\n",
    "                # Limiting validation data.\n",
    "                if batch_idx * BATCHSIZE >= N_VALID_EXAMPLES: #TODO: discuss getting rid of this \n",
    "                    break\n",
    "                data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "                output = model(data)\n",
    "                # Get the index of the max log-probability.\n",
    "                pred = output.argmax(dim=1, keepdim=True)\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        accuracy = correct / min(len(valid_loader.dataset), N_VALID_EXAMPLES)\n",
    "        \n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy \n",
    "            best_epoch = epoch\n",
    "            if epochs_since_improvement > 0:\n",
    "                if best_patience < epochs_since_improvement:\n",
    "                    best_patience = epochs_since_improvement\n",
    "                epochs_since_improvement = 0\n",
    "        else:\n",
    "            epochs_since_improvement += 1\n",
    "\n",
    "        trial.report(accuracy, epoch)\n",
    "\n",
    "        # Handle pruning based on the intermediate value.\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    print(f\"Best patience value: {best_patience}\")\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-02 17:32:43,250] A new study created in memory with name: no-name-b226c67d-51e1-41f2-ae1e-6f94ce5f5428\n",
      "[I 2023-11-02 17:32:52,025] Trial 0 finished with value: 0.9896484375 and parameters: {'layer1_channels_exp': 5, 'layer2_channels_exp': 6, 'dropout_p:': 0.04954806781113186, 'optimizer': 'Adam', 'weight_decay': 0.00012301254698047313, 'lr': 0.0012628735461133397}. Best is trial 0 with value: 0.9896484375.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best patience value: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-02 17:33:03,139] Trial 1 finished with value: 0.9875 and parameters: {'layer1_channels_exp': 4, 'layer2_channels_exp': 8, 'dropout_p:': 0.006427139479331512, 'optimizer': 'Adam', 'weight_decay': 0.0002795061721743844, 'lr': 0.0032932878931397585}. Best is trial 0 with value: 0.9896484375.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best patience value: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-02 17:33:11,228] Trial 2 finished with value: 0.900390625 and parameters: {'layer1_channels_exp': 3, 'layer2_channels_exp': 6, 'dropout_p:': 0.07197006756324066, 'optimizer': 'RMSprop', 'weight_decay': 0.005903084532832784, 'lr': 1.3596397305491095e-05}. Best is trial 0 with value: 0.9896484375.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best patience value: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-02 17:33:22,355] Trial 3 finished with value: 0.97578125 and parameters: {'layer1_channels_exp': 4, 'layer2_channels_exp': 8, 'dropout_p:': 0.004202348924556831, 'optimizer': 'RMSprop', 'weight_decay': 0.0010881457396317786, 'lr': 3.686437074860214e-05}. Best is trial 0 with value: 0.9896484375.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best patience value: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-02 17:33:31,082] Trial 4 finished with value: 0.9837890625 and parameters: {'layer1_channels_exp': 3, 'layer2_channels_exp': 7, 'dropout_p:': 0.04639268706575398, 'optimizer': 'Adam', 'weight_decay': 0.00013794676534991158, 'lr': 0.0004815240235843506}. Best is trial 0 with value: 0.9896484375.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best patience value: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-02 17:33:32,559] Trial 5 pruned. \n",
      "[I 2023-11-02 17:33:35,766] Trial 6 pruned. \n",
      "[I 2023-11-02 17:33:36,669] Trial 7 pruned. \n",
      "[I 2023-11-02 17:33:37,577] Trial 8 pruned. \n",
      "[I 2023-11-02 17:33:40,744] Trial 9 pruned. \n",
      "[I 2023-11-02 17:33:43,418] Trial 10 pruned. \n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100, timeout=600)\n",
    "\n",
    "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignment2-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
